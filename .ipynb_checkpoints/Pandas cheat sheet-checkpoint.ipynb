{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Commands\n",
    "#Shift+return to run code\n",
    "#Cell > markdown\n",
    "#To comment code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's in my workspace?\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv file\n",
    "#builds a dataframe from csv file\n",
    "marketing = pd.read_csv('datasets/marketing.csv')\n",
    "\n",
    "# making data frame  \n",
    "sales = pd.read_csv(\"https://assets.datacamp.com/production/repositories/5386/datasets/5110afec30fc30bc5f3cf67b188d1513c3d6d940/sales_subset.csv\")  \n",
    "    \n",
    "homelessness = pd.read_csv(\"https://assets.datacamp.com/production/repositories/5386/datasets/1a0ab2e8557930ec06473c16521874e516a216ae/homelessness.csv\")  \n",
    "   \n",
    "temperatures = pd.read_csv(\"https://assets.datacamp.com/production/repositories/5386/datasets/47f5fde162bae3549ca7d5c26fb4c4639f100f28/temperatures.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date_served</th>\n",
       "      <th>marketing_channel</th>\n",
       "      <th>variant</th>\n",
       "      <th>converted</th>\n",
       "      <th>language_displayed</th>\n",
       "      <th>language_preferred</th>\n",
       "      <th>age_group</th>\n",
       "      <th>date_subscribed</th>\n",
       "      <th>date_canceled</th>\n",
       "      <th>subscribing_channel</th>\n",
       "      <th>is_retained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a100000029</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>0-18 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a100000030</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>19-24 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a100000031</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>24-30 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a100000032</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>30-36 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a100000033</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>36-45 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id date_served marketing_channel          variant converted  \\\n",
       "0  a100000029      1/1/18         House Ads  personalization      True   \n",
       "1  a100000030      1/1/18         House Ads  personalization      True   \n",
       "2  a100000031      1/1/18         House Ads  personalization      True   \n",
       "3  a100000032      1/1/18         House Ads  personalization      True   \n",
       "4  a100000033      1/1/18         House Ads  personalization      True   \n",
       "\n",
       "  language_displayed language_preferred    age_group date_subscribed  \\\n",
       "0            English            English   0-18 years          1/1/18   \n",
       "1            English            English  19-24 years          1/1/18   \n",
       "2            English            English  24-30 years          1/1/18   \n",
       "3            English            English  30-36 years          1/1/18   \n",
       "4            English            English  36-45 years          1/1/18   \n",
       "\n",
       "  date_canceled subscribing_channel is_retained  \n",
       "0           NaN           House Ads        True  \n",
       "1           NaN           House Ads        True  \n",
       "2           NaN           House Ads        True  \n",
       "3           NaN           House Ads        True  \n",
       "4           NaN           House Ads        True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#examine the data\n",
    "display(marketing.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10037 entries, 0 to 10036\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   user_id              10037 non-null  object\n",
      " 1   date_served          10021 non-null  object\n",
      " 2   marketing_channel    10022 non-null  object\n",
      " 3   variant              10037 non-null  object\n",
      " 4   converted            10022 non-null  object\n",
      " 5   language_displayed   10037 non-null  object\n",
      " 6   language_preferred   10037 non-null  object\n",
      " 7   age_group            10037 non-null  object\n",
      " 8   date_subscribed      1856 non-null   object\n",
      " 9   date_canceled        577 non-null    object\n",
      " 10  subscribing_channel  1856 non-null   object\n",
      " 11  is_retained          1856 non-null   object\n",
      "dtypes: object(12)\n",
      "memory usage: 941.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check column data types and non-missing values\n",
    "print(marketing.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0           0      1    A           1  2010-02-05      24924.50       False   \n",
      "1           1      1    A           1  2010-03-05      21827.90       False   \n",
      "2           2      1    A           1  2010-04-02      57258.43       False   \n",
      "3           3      1    A           1  2010-05-07      17413.94       False   \n",
      "4           4      1    A           1  2010-06-04      17558.09       False   \n",
      "\n",
      "   temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0       5.727778              0.679451         8.106  \n",
      "1       8.055556              0.693452         8.106  \n",
      "2      16.816667              0.718284         7.808  \n",
      "3      22.527778              0.748928         7.808  \n",
      "4      27.050000              0.714586         7.808  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10774 entries, 0 to 10773\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Unnamed: 0            10774 non-null  int64  \n",
      " 1   store                 10774 non-null  int64  \n",
      " 2   type                  10774 non-null  object \n",
      " 3   department            10774 non-null  int64  \n",
      " 4   date                  10774 non-null  object \n",
      " 5   weekly_sales          10774 non-null  float64\n",
      " 6   is_holiday            10774 non-null  bool   \n",
      " 7   temperature_c         10774 non-null  float64\n",
      " 8   fuel_price_usd_per_l  10774 non-null  float64\n",
      " 9   unemployment          10774 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(3), object(2)\n",
      "memory usage: 768.2+ KB\n",
      "None\n",
      "23843.950148505668\n",
      "12049.064999999999\n"
     ]
    }
   ],
   "source": [
    "#Mean and median\n",
    "\n",
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales[\"weekly_sales\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-10-26\n",
      "2010-02-05\n"
     ]
    }
   ],
   "source": [
    "#Summarizing dates\n",
    "# Print the maximum of the date column\n",
    "print(sales['date'].max())\n",
    "# Print the minimum of the date column\n",
    "print(sales['date'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.583333333333336\n"
     ]
    }
   ],
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# Print IQR of the temperature_c column\n",
    "print(sales['temperature_c'].agg(iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  store type  department        date  weekly_sales  \\\n",
      "0              0      1    A           1  2010-02-05      24924.50   \n",
      "901          901      2    A           1  2010-02-05      35034.06   \n",
      "1798        1798      4    A           1  2010-02-05      38724.42   \n",
      "2699        2699      6    A           1  2010-02-05      25619.00   \n",
      "3593        3593     10    B           1  2010-02-05      40212.84   \n",
      "\n",
      "      is_holiday  temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0          False       5.727778              0.679451         8.106  \n",
      "901        False       4.550000              0.679451         8.324  \n",
      "1798       False       6.533333              0.686319         8.623  \n",
      "2699       False       4.683333              0.679451         7.259  \n",
      "3593       False      12.411111              0.782478         9.765  \n",
      "    Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0            0      1    A           1  2010-02-05      24924.50       False   \n",
      "12          12      1    A           2  2010-02-05      50605.27       False   \n",
      "24          24      1    A           3  2010-02-05      13740.12       False   \n",
      "36          36      1    A           4  2010-02-05      39954.04       False   \n",
      "48          48      1    A           5  2010-02-05      32229.38       False   \n",
      "\n",
      "    temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0        5.727778              0.679451         8.106  \n",
      "12       5.727778              0.679451         8.106  \n",
      "24       5.727778              0.679451         8.106  \n",
      "36       5.727778              0.679451         8.106  \n",
      "48       5.727778              0.679451         8.106  \n",
      "      Unnamed: 0  store type  department        date  weekly_sales  \\\n",
      "498          498      1    A          45  2010-09-10         11.47   \n",
      "691          691      1    A          77  2011-11-25       1431.00   \n",
      "2315        2315      4    A          47  2010-02-12        498.00   \n",
      "6735        6735     19    A          39  2012-09-07         13.41   \n",
      "6810        6810     19    A          47  2010-12-31       -449.00   \n",
      "6815        6815     19    A          47  2012-02-10         15.00   \n",
      "6820        6820     19    A          48  2011-09-09        197.00   \n",
      "\n",
      "      is_holiday  temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "498         True      25.938889              0.677602         7.787  \n",
      "691         True      15.633333              0.854861         7.866  \n",
      "2315        True      -1.755556              0.679715         8.623  \n",
      "6735        True      22.333333              1.076766         8.193  \n",
      "6810        True      -1.861111              0.881278         8.067  \n",
      "6815        True       0.338889              1.010723         7.943  \n",
      "6820        True      20.155556              1.038197         7.806  \n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales[\"is_holiday\"]==True].drop_duplicates(subset=\"date\")\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    11\n",
      "B     1\n",
      "Name: type, dtype: int64\n",
      "A    0.916667\n",
      "B    0.083333\n",
      "Name: type, dtype: float64\n",
      "1     12\n",
      "55    12\n",
      "72    12\n",
      "71    12\n",
      "67    12\n",
      "      ..\n",
      "37    10\n",
      "48     8\n",
      "50     6\n",
      "39     4\n",
      "43     2\n",
      "Name: department, Length: 80, dtype: int64\n",
      "1     0.012917\n",
      "55    0.012917\n",
      "72    0.012917\n",
      "71    0.012917\n",
      "67    0.012917\n",
      "        ...   \n",
      "37    0.010764\n",
      "48    0.008611\n",
      "50    0.006459\n",
      "39    0.004306\n",
      "43    0.002153\n",
      "Name: department, Length: 80, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Counting categorical variables\n",
    "# Count the number of stores of each type\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types[\"type\"].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of each department number and sort\n",
    "dept_counts_sorted = store_depts[\"department\"].value_counts(sort=True)\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of departments of each number and sort\n",
    "dept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percent of sales occurred at each store type?\n",
    "While .groupby() is useful, you can calculate grouped summary statistics without it.\n",
    "\n",
    "Walmart distinguishes three types of stores: \"supercenters,\" \"discount stores,\" and \"neighborhood markets,\" encoded in this dataset as type \"A,\" \"B,\" and \"C.\" In this exercise, you'll calculate the total sales made at each store type, without using .groupby(). You can then use these numbers to see what proportion of Walmart's total sales were made at each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc total weekly sales\n",
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "A    0.909775\n",
      "B    0.090225\n",
      "Name: weekly_sales, dtype: float64\n",
      "type  is_holiday\n",
      "A     False         2.336927e+08\n",
      "      True          2.360181e+04\n",
      "B     False         2.317678e+07\n",
      "      True          1.621410e+03\n",
      "Name: weekly_sales, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Calculations with .groupby()\n",
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = sales_by_type / sum(sales_by_type)\n",
    "print(sales_propn_by_type)\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby([\"type\", \"is_holiday\"])[\"weekly_sales\"].sum()\n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple grouped summaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in this chapter, you saw that the .agg() method is useful to compute multiple statistics on multiple variables. It also works with grouped data. NumPy, which is imported as np, has many different summary statistics functions, including: np.min, np.max, np.mean, and np.median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        amin       amax          mean    median\n",
      "type                                           \n",
      "A    -1098.0  293966.05  23674.667242  11943.92\n",
      "B     -798.0  232558.51  25696.678370  13336.08\n",
      "     unemployment                         fuel_price_usd_per_l            \\\n",
      "             amin   amax      mean median                 amin      amax   \n",
      "type                                                                       \n",
      "A           3.879  8.992  7.972611  8.067             0.664129  1.107410   \n",
      "B           7.170  9.765  9.279323  9.199             0.760023  1.107674   \n",
      "\n",
      "                          \n",
      "          mean    median  \n",
      "type                      \n",
      "A     0.744619  0.735455  \n",
      "B     0.805858  0.803348  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/r2b0spdj3mz03127m_zx742r0000gn/T/ipykernel_33468/3728837400.py:11: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  unemp_fuel_stats = sales.groupby(\"type\")[\"unemployment\", \"fuel_price_usd_per_l\"].agg([np.min, np.max, np.mean, np.median])\n"
     ]
    }
   ],
   "source": [
    "# Import numpy with the alias np\n",
    "import numpy as np\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby(\"type\")[\"unemployment\", \"fuel_price_usd_per_l\"].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot tables are the standard way of aggregating data in spreadsheets. In pandas, pivot tables are essentially just another way of performing grouped calculations. That is, the .pivot_table() method is just an alternative to .groupby().\n",
    "\n",
    "In this exercise, you'll perform calculations using .pivot_table() to replicate the calculations you performed in the last lesson using .groupby()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      weekly_sales\n",
      "type              \n",
      "A     23674.667242\n",
      "B     25696.678370\n"
     ]
    }
   ],
   "source": [
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\")\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              mean       median\n",
      "      weekly_sales weekly_sales\n",
      "type                           \n",
      "A     23674.667242     11943.92\n",
      "B     25696.678370     13336.08\n"
     ]
    }
   ],
   "source": [
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\", aggfunc=[np.mean, np.median])\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_holiday         False       True\n",
      "type                               \n",
      "A           23768.583523  590.04525\n",
      "B           25751.980533  810.70500\n"
     ]
    }
   ],
   "source": [
    "# Pivot for mean weekly_sales by store type and holiday \n",
    "mean_sales_by_type_holiday = sales.pivot_table(values=\"weekly_sales\", index=\"type\", columns = \"is_holiday\")\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .pivot_table() method has several useful arguments, including fill_value and margins.\n",
    "\n",
    "fill_value replaces missing values with a real value (known as imputation). What to replace missing values with is a topic big enough to have its own course (Dealing with Missing Data in Python), but the simplest thing to do is to substitute a dummy value.\n",
    "margins is a shortcut for when you pivoted by two variables, but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                    A              B\n",
      "department                              \n",
      "1            30961.725379   44050.626667\n",
      "2            67600.158788  112958.526667\n",
      "3            17160.002955   30580.655000\n",
      "4            44285.399091   51219.654167\n",
      "5            34821.011364   63236.875000\n",
      "...                   ...            ...\n",
      "95          123933.787121   77082.102500\n",
      "96           21367.042857    9528.538333\n",
      "97           28471.266970    5828.873333\n",
      "98           12875.423182     217.428333\n",
      "99             379.123659       0.000000\n",
      "\n",
      "[80 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                   A              B           All\n",
      "department                                           \n",
      "1           30961.725379   44050.626667  32052.467153\n",
      "2           67600.158788  112958.526667  71380.022778\n",
      "3           17160.002955   30580.655000  18278.390625\n",
      "4           44285.399091   51219.654167  44863.253681\n",
      "5           34821.011364   63236.875000  37189.000000\n",
      "...                  ...            ...           ...\n",
      "96          21367.042857    9528.538333  20337.607681\n",
      "97          28471.266970    5828.873333  26584.400833\n",
      "98          12875.423182     217.428333  11820.590278\n",
      "99            379.123659       0.000000    379.123659\n",
      "All         23674.667242   25696.678370  23843.950149\n",
      "\n",
      "[81 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0, margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting and removing indexes\n",
    "pandas allows you to designate columns as an index. This enables cleaner code when taking subsets (as well as providing more efficient lookup under some circumstances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0        date     city        country  avg_temp_c\n",
      "0               0  2000-01-01  Abidjan  Côte D'Ivoire      27.293\n",
      "1               1  2000-02-01  Abidjan  Côte D'Ivoire      27.685\n",
      "2               2  2000-03-01  Abidjan  Côte D'Ivoire      29.061\n",
      "3               3  2000-04-01  Abidjan  Côte D'Ivoire      28.162\n",
      "4               4  2000-05-01  Abidjan  Côte D'Ivoire      27.547\n",
      "...           ...         ...      ...            ...         ...\n",
      "16495       16495  2013-05-01     Xian          China      18.979\n",
      "16496       16496  2013-06-01     Xian          China      23.522\n",
      "16497       16497  2013-07-01     Xian          China      25.251\n",
      "16498       16498  2013-08-01     Xian          China      24.528\n",
      "16499       16499  2013-09-01     Xian          China         NaN\n",
      "\n",
      "[16500 rows x 5 columns]\n",
      "         Unnamed: 0        date        country  avg_temp_c\n",
      "city                                                      \n",
      "Abidjan           0  2000-01-01  Côte D'Ivoire      27.293\n",
      "Abidjan           1  2000-02-01  Côte D'Ivoire      27.685\n",
      "Abidjan           2  2000-03-01  Côte D'Ivoire      29.061\n",
      "Abidjan           3  2000-04-01  Côte D'Ivoire      28.162\n",
      "Abidjan           4  2000-05-01  Côte D'Ivoire      27.547\n",
      "...             ...         ...            ...         ...\n",
      "Xian          16495  2013-05-01          China      18.979\n",
      "Xian          16496  2013-06-01          China      23.522\n",
      "Xian          16497  2013-07-01          China      25.251\n",
      "Xian          16498  2013-08-01          China      24.528\n",
      "Xian          16499  2013-09-01          China         NaN\n",
      "\n",
      "[16500 rows x 4 columns]\n",
      "          city  Unnamed: 0        date        country  avg_temp_c\n",
      "0      Abidjan           0  2000-01-01  Côte D'Ivoire      27.293\n",
      "1      Abidjan           1  2000-02-01  Côte D'Ivoire      27.685\n",
      "2      Abidjan           2  2000-03-01  Côte D'Ivoire      29.061\n",
      "3      Abidjan           3  2000-04-01  Côte D'Ivoire      28.162\n",
      "4      Abidjan           4  2000-05-01  Côte D'Ivoire      27.547\n",
      "...        ...         ...         ...            ...         ...\n",
      "16495     Xian       16495  2013-05-01          China      18.979\n",
      "16496     Xian       16496  2013-06-01          China      23.522\n",
      "16497     Xian       16497  2013-07-01          China      25.251\n",
      "16498     Xian       16498  2013-08-01          China      24.528\n",
      "16499     Xian       16499  2013-09-01          China         NaN\n",
      "\n",
      "[16500 rows x 5 columns]\n",
      "       Unnamed: 0        date        country  avg_temp_c\n",
      "0               0  2000-01-01  Côte D'Ivoire      27.293\n",
      "1               1  2000-02-01  Côte D'Ivoire      27.685\n",
      "2               2  2000-03-01  Côte D'Ivoire      29.061\n",
      "3               3  2000-04-01  Côte D'Ivoire      28.162\n",
      "4               4  2000-05-01  Côte D'Ivoire      27.547\n",
      "...           ...         ...            ...         ...\n",
      "16495       16495  2013-05-01          China      18.979\n",
      "16496       16496  2013-06-01          China      23.522\n",
      "16497       16497  2013-07-01          China      25.251\n",
      "16498       16498  2013-08-01          China      24.528\n",
      "16499       16499  2013-09-01          China         NaN\n",
      "\n",
      "[16500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Look at temperatures\n",
    "print(temperatures)\n",
    "\n",
    "# Index temperatures by city\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "\n",
    "# Look at temperatures_ind\n",
    "print(temperatures_ind)\n",
    "\n",
    "# Reset the index, keeping its contents\n",
    "print(temperatures_ind.reset_index())\n",
    "\n",
    "# Reset the index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting with .loc[]\n",
    "The killer feature for indexes is .loc[]: a subsetting method that accepts index values. When you pass it a single argument, it will take a subset of rows.\n",
    "\n",
    "The code for subsetting using .loc[] can be easier to read than standard square bracket subsetting, which can make your code less burdensome to maintain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0        date              city country  avg_temp_c\n",
      "10725       10725  2000-01-01            Moscow  Russia      -7.313\n",
      "10726       10726  2000-02-01            Moscow  Russia      -3.551\n",
      "10727       10727  2000-03-01            Moscow  Russia      -1.661\n",
      "10728       10728  2000-04-01            Moscow  Russia      10.096\n",
      "10729       10729  2000-05-01            Moscow  Russia      10.357\n",
      "...           ...         ...               ...     ...         ...\n",
      "13360       13360  2013-05-01  Saint Petersburg  Russia      12.355\n",
      "13361       13361  2013-06-01  Saint Petersburg  Russia      17.185\n",
      "13362       13362  2013-07-01  Saint Petersburg  Russia      17.234\n",
      "13363       13363  2013-08-01  Saint Petersburg  Russia      17.153\n",
      "13364       13364  2013-09-01  Saint Petersburg  Russia         NaN\n",
      "\n",
      "[330 rows x 5 columns]\n",
      "                  Unnamed: 0        date country  avg_temp_c\n",
      "city                                                        \n",
      "Moscow                 10725  2000-01-01  Russia      -7.313\n",
      "Moscow                 10726  2000-02-01  Russia      -3.551\n",
      "Moscow                 10727  2000-03-01  Russia      -1.661\n",
      "Moscow                 10728  2000-04-01  Russia      10.096\n",
      "Moscow                 10729  2000-05-01  Russia      10.357\n",
      "...                      ...         ...     ...         ...\n",
      "Saint Petersburg       13360  2013-05-01  Russia      12.355\n",
      "Saint Petersburg       13361  2013-06-01  Russia      17.185\n",
      "Saint Petersburg       13362  2013-07-01  Russia      17.234\n",
      "Saint Petersburg       13363  2013-08-01  Russia      17.153\n",
      "Saint Petersburg       13364  2013-09-01  Russia         NaN\n",
      "\n",
      "[330 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures[\"city\"].isin(cities)])\n",
    "\n",
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[cities])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting multi-level indexes\n",
    "Indexes can also be made out of multiple columns, forming a multi-level index (sometimes called a hierarchical index). There is a trade-off to using these.\n",
    "\n",
    "The benefit is that multi-level indexes make it more natural to reason about nested categorical variables. For example, in a clinical trial, you might have control and treatment groups. Then each test subject belongs to one or another group, and we can say that a test subject is nested inside the treatment group. Similarly, in the temperature dataset, the city is located in the country, so we can say a city is nested inside the country.\n",
    "\n",
    "The main downside is that the code for manipulating indexes is different from the code for manipulating columns, so you have to learn two syntaxes and keep track of how your data is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Unnamed: 0        date  avg_temp_c\n",
      "country  city                                              \n",
      "Brazil   Rio De Janeiro       12540  2000-01-01      25.974\n",
      "         Rio De Janeiro       12541  2000-02-01      26.699\n",
      "         Rio De Janeiro       12542  2000-03-01      26.270\n",
      "         Rio De Janeiro       12543  2000-04-01      25.750\n",
      "         Rio De Janeiro       12544  2000-05-01      24.356\n",
      "...                             ...         ...         ...\n",
      "Pakistan Lahore                8575  2013-05-01      33.457\n",
      "         Lahore                8576  2013-06-01      34.456\n",
      "         Lahore                8577  2013-07-01      33.279\n",
      "         Lahore                8578  2013-08-01      31.511\n",
      "         Lahore                8579  2013-09-01         NaN\n",
      "\n",
      "[330 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
    "\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"),(\"Pakistan\", \"Lahore\")]\n",
    "\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.loc[rows_to_keep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting by index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Unnamed: 0        date  avg_temp_c\n",
      "country     city                                      \n",
      "Afghanistan Kabul         7260  2000-01-01       3.326\n",
      "            Kabul         7261  2000-02-01       3.454\n",
      "            Kabul         7262  2000-03-01       9.612\n",
      "            Kabul         7263  2000-04-01      17.925\n",
      "            Kabul         7264  2000-05-01      24.658\n",
      "...                        ...         ...         ...\n",
      "Zimbabwe    Harare        5605  2013-05-01      18.298\n",
      "            Harare        5606  2013-06-01      17.020\n",
      "            Harare        5607  2013-07-01      16.299\n",
      "            Harare        5608  2013-08-01      19.232\n",
      "            Harare        5609  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n",
      "                       Unnamed: 0        date  avg_temp_c\n",
      "country       city                                       \n",
      "Côte D'Ivoire Abidjan           0  2000-01-01      27.293\n",
      "              Abidjan           1  2000-02-01      27.685\n",
      "              Abidjan           2  2000-03-01      29.061\n",
      "              Abidjan           3  2000-04-01      28.162\n",
      "              Abidjan           4  2000-05-01      27.547\n",
      "...                           ...         ...         ...\n",
      "China         Xian          16495  2013-05-01      18.979\n",
      "              Xian          16496  2013-06-01      23.522\n",
      "              Xian          16497  2013-07-01      25.251\n",
      "              Xian          16498  2013-08-01      24.528\n",
      "              Xian          16499  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n",
      "                    Unnamed: 0        date  avg_temp_c\n",
      "country     city                                      \n",
      "Afghanistan Kabul         7260  2000-01-01       3.326\n",
      "            Kabul         7261  2000-02-01       3.454\n",
      "            Kabul         7262  2000-03-01       9.612\n",
      "            Kabul         7263  2000-04-01      17.925\n",
      "            Kabul         7264  2000-05-01      24.658\n",
      "...                        ...         ...         ...\n",
      "Zimbabwe    Harare        5605  2013-05-01      18.298\n",
      "            Harare        5606  2013-06-01      17.020\n",
      "            Harare        5607  2013-07-01      16.299\n",
      "            Harare        5608  2013-08-01      19.232\n",
      "            Harare        5609  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index())\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level = \"city\"))\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level=[\"country\", \"city\"], ascending=[True, False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing index values\n",
    "\n",
    "\n",
    "Slicing lets you select consecutive elements of an object using first:last syntax. DataFrames can be sliced by index values or by row/column number; we'll start with the first case. This involves slicing inside the .loc[] method.\n",
    "\n",
    "Compared to slicing lists, there are a few things to remember.\n",
    "\n",
    "You can only slice an index if the index is sorted (using .sort_index()).\n",
    "To slice at the outer level, first and last can be strings.\n",
    "To slice at inner levels, first and last should be tuples.\n",
    "If you pass a single slice to .loc[], it will slice the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Unnamed: 0        date  avg_temp_c\n",
      "country  city                                                \n",
      "Pakistan Faisalabad              4785  2000-01-01      12.792\n",
      "         Faisalabad              4786  2000-02-01      14.339\n",
      "         Faisalabad              4787  2000-03-01      20.309\n",
      "         Faisalabad              4788  2000-04-01      29.072\n",
      "         Faisalabad              4789  2000-05-01      34.845\n",
      "...                               ...         ...         ...\n",
      "Russia   Saint Petersburg       13360  2013-05-01      12.355\n",
      "         Saint Petersburg       13361  2013-06-01      17.185\n",
      "         Saint Petersburg       13362  2013-07-01      17.234\n",
      "         Saint Petersburg       13363  2013-08-01      17.153\n",
      "         Saint Petersburg       13364  2013-09-01         NaN\n",
      "\n",
      "[1155 rows x 3 columns]\n",
      "                    Unnamed: 0        date  avg_temp_c\n",
      "country city                                          \n",
      "Mexico  Mexico           10230  2000-01-01      12.694\n",
      "        Mexico           10231  2000-02-01      14.677\n",
      "        Mexico           10232  2000-03-01      17.376\n",
      "        Mexico           10233  2000-04-01      18.294\n",
      "        Mexico           10234  2000-05-01      18.562\n",
      "...                        ...         ...         ...\n",
      "Morocco Casablanca        3130  2013-05-01      19.217\n",
      "        Casablanca        3131  2013-06-01      23.649\n",
      "        Casablanca        3132  2013-07-01      27.488\n",
      "        Casablanca        3133  2013-08-01      27.952\n",
      "        Casablanca        3134  2013-09-01         NaN\n",
      "\n",
      "[330 rows x 3 columns]\n",
      "                 Unnamed: 0        date  avg_temp_c\n",
      "country  city                                      \n",
      "Pakistan Lahore        8415  2000-01-01      12.792\n",
      "         Lahore        8416  2000-02-01      14.339\n",
      "         Lahore        8417  2000-03-01      20.309\n",
      "         Lahore        8418  2000-04-01      29.072\n",
      "         Lahore        8419  2000-05-01      34.845\n",
      "...                     ...         ...         ...\n",
      "Russia   Moscow       10885  2013-05-01      16.152\n",
      "         Moscow       10886  2013-06-01      18.718\n",
      "         Moscow       10887  2013-07-01      18.136\n",
      "         Moscow       10888  2013-08-01      17.485\n",
      "         Moscow       10889  2013-09-01         NaN\n",
      "\n",
      "[660 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "\n",
    "# Subset rows from Pakistan to Russia\n",
    "print(temperatures_srt.loc[\"Pakistan\": \"Russia\"])\n",
    "\n",
    "# Try to subset rows from Lahore to Moscow\n",
    "print(temperatures_srt.loc[\"Lahore\": \"Moscow\"])\n",
    "\n",
    "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(temperatures_srt.loc[(\"Pakistan\", \"Lahore\"): (\"Russia\",\"Moscow\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "#print a data type of a single column \n",
    "print(marketing['converted'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the date type of a column\n",
    "marketing['converted'] = marketing['converted'].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bool\n"
     ]
    }
   ],
   "source": [
    "#check results - should now be boolean\n",
    "print(marketing['converted'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new boolean columns\n",
    "marketing['is_house_ads']  = np.where(marketing['marketing_channel'] == 'House Ads', True, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "Name: is_house_ads, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(marketing.is_house_ads.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>date_served</th>\n",
       "      <th>marketing_channel</th>\n",
       "      <th>variant</th>\n",
       "      <th>converted</th>\n",
       "      <th>language_displayed</th>\n",
       "      <th>language_preferred</th>\n",
       "      <th>age_group</th>\n",
       "      <th>date_subscribed</th>\n",
       "      <th>date_canceled</th>\n",
       "      <th>subscribing_channel</th>\n",
       "      <th>is_retained</th>\n",
       "      <th>is_house_ads</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a100000029</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>0-18 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a100000030</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>19-24 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a100000031</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>24-30 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a100000032</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>30-36 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a100000033</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>personalization</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>English</td>\n",
       "      <td>36-45 years</td>\n",
       "      <td>1/1/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>House Ads</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id date_served marketing_channel          variant  converted  \\\n",
       "0  a100000029      1/1/18         House Ads  personalization       True   \n",
       "1  a100000030      1/1/18         House Ads  personalization       True   \n",
       "2  a100000031      1/1/18         House Ads  personalization       True   \n",
       "3  a100000032      1/1/18         House Ads  personalization       True   \n",
       "4  a100000033      1/1/18         House Ads  personalization       True   \n",
       "\n",
       "  language_displayed language_preferred    age_group date_subscribed  \\\n",
       "0            English            English   0-18 years          1/1/18   \n",
       "1            English            English  19-24 years          1/1/18   \n",
       "2            English            English  24-30 years          1/1/18   \n",
       "3            English            English  30-36 years          1/1/18   \n",
       "4            English            English  36-45 years          1/1/18   \n",
       "\n",
       "  date_canceled subscribing_channel is_retained  is_house_ads  id  \n",
       "0           NaN           House Ads        True          True   0  \n",
       "1           NaN           House Ads        True          True   0  \n",
       "2           NaN           House Ads        True          True   0  \n",
       "3           NaN           House Ads        True          True   0  \n",
       "4           NaN           House Ads        True          True   0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Broadcasting - a new column can be created on the fly\n",
    "marketing['id'] = 0\n",
    "\n",
    "#check results\n",
    "marketing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new columns\n",
    "\t\ta. Adding new columns that derive information from existing data or based on domain knowledge is known as Feature Engineering. Even in relatively simple datasets, there are always new characteristics you could pull out to create a more in-depth analysis.\n",
    "\t\tb. One of the most critical skills a data scientist needs to learn is how to identify opportunities for feature engineering.\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for channels\n",
    "channel_dict = {\"House Ads\": 1, \"Instagram\": 2, \n",
    "                \"Facebook\": 3, \"Email\": 4, \"Push\": 5}\n",
    "\n",
    "# Map the channel to a channel code\n",
    "marketing['channel_code'] = marketing['subscribing_channel'].map(channel_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Yes\n",
      "1    Yes\n",
      "2    Yes\n",
      "Name: is_correct_lang, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Add the new column is_correct_lang\n",
    "marketing['is_correct_lang']  = np.where(marketing['language_displayed'] == marketing['language_preferred'], 'Yes', 'No'\n",
    ")\n",
    "\n",
    "print(marketing.is_correct_lang.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the date columns in the marketing DataFrame are being incorrectly read as objects. We need to convert these columns to date columns to be able to use Python and pandas' robust date manipulation and formatting capabilities.\n",
    "\n",
    "\n",
    "In this exercise, you will practice reading the CSV with proper date columns and create a day of the week columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import marketing.csv with date columns\n",
    "marketing = pd.read_csv('marketing.csv', parse_dates=['date_served', \n",
    "'date_subscribed', 'date_canceled']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing['day_served'] = marketing['date_served'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a DoW column\n",
    "marketing['DoW'] = marketing['date_subscribed'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial exploratory analysis\n",
    "\t\ta. How many unique users see our assets each day\n",
    "\t\t\ti. need to use group by method\n",
    "                  1) We pass \"date served\" as the argument to groupby()\n",
    "\t\t\t  2) We select the userid column outside of the groupby\n",
    "                3.And use nunique method to count the number of unique users each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_served\n",
      "2018-01-01    362\n",
      "2018-01-02    374\n",
      "2018-01-03    348\n",
      "2018-01-04    323\n",
      "2018-01-05    319\n",
      "Name: user_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group by date_served and count number of unique user_id's\n",
    "daily_users = marketing.groupby(['date_served'])['user_id'].nunique()\n",
    " \n",
    "# Print head of daily_users\n",
    "print(daily_users.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-60d43b3a19d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Include a title and y-axis label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Daily users'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of users'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#Visualizing daily marketing reach\n",
    "\n",
    "# Plot daily_subscribers\n",
    "daily_users.plot()\n",
    " \n",
    "# Include a title and y-axis label\n",
    "plt.title('Daily users')\n",
    "plt.ylabel('Number of users')\n",
    " \n",
    "# Rotate the x-axis labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    " \n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.89 %\n"
     ]
    }
   ],
   "source": [
    "#Calculating conversion rate\n",
    "\n",
    "# Calculate the number of people we marketed to\n",
    "total = marketing['user_id'].nunique()\n",
    " \n",
    "# Calculate the number of people who subscribed\n",
    "subscribers = marketing[marketing['converted']== True]['user_id'].nunique()\n",
    " \n",
    "# Calculate the conversion rate\n",
    "conversion_rate = subscribers/total\n",
    "print(round(conversion_rate*100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.8 %\n"
     ]
    }
   ],
   "source": [
    "#Calculating retention rate\n",
    "\n",
    "# Calculate the number of subscribers\n",
    "total_subscribers = marketing[marketing['converted']==True]['user_id'].nunique()\n",
    "\n",
    "# Calculate the number of people who remained subscribed\n",
    "retained = marketing[marketing['is_retained']==True]['user_id'].nunique()\n",
    "\n",
    "# Calculate the retention rate\n",
    "retention_rate = retained/total_subscribers\n",
    "print(round(retention_rate*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English speaker conversion rate: 12.92 %\n"
     ]
    }
   ],
   "source": [
    "#Customer Segmentation\n",
    "# Isolate english speakers\n",
    "english_speakers = marketing[marketing['language_displayed'] == 'English']\n",
    "\n",
    "# Calculate the total number of English speaking users\n",
    "total = marketing[marketing['language_displayed'] == 'English']['user_id'].nunique()\n",
    "\n",
    "# Calculate the number of English speakers who converted\n",
    "subscribers = english_speakers[english_speakers['converted']==True]['user_id'].nunique()\n",
    "\n",
    "# Calculate conversion rate\n",
    "conversion_rate = subscribers/total\n",
    "print('English speaker conversion rate:', round(conversion_rate*100,2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language_displayed\n",
      "Arabic     0.500000\n",
      "English    0.129167\n",
      "German     0.716216\n",
      "Spanish    0.200000\n",
      "Name: user_id, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Comparing language conversion rate (ii)\n",
    "#Group by language_displayed and count unique users\n",
    "total = marketing.groupby(['language_displayed'])['user_id'].nunique()\n",
    "\n",
    "# Group by language_displayed and count unique conversions\n",
    "subscribers = marketing[marketing['converted']==True].groupby(['language_displayed'])['user_id'].nunique()\n",
    "\n",
    "# Calculate the conversion rate for all languages\n",
    "language_conversion_rate = subscribers/total\n",
    "print(language_conversion_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
